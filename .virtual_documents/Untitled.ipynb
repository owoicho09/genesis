


import os
import sys
import re
import time
import csv
import requests
from bs4 import BeautifulSoup
from googlesearch_python import search



from googlesearch import search
import requests
import re
from bs4 import BeautifulSoup

EMAIL_REGEX = r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}"

query = '"fitness coach" "@gmail.com" site:linkedin.com'
urls = search(query, num_results=50)

emails_found = []

for url in urls:
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        text = soup.get_text()
        emails = re.findall(EMAIL_REGEX, text)
        for email in set(emails):
            print(f"üß† {email}")
            emails_found.append(email)
    except Exception as e:
        print(f"‚ùå {url} ‚Äî {e}")



class GoogleDorkScraper:
    def __init__(self, niche, limit=20):
        self.niche = niche
        self.limit = limit
        self.scraped_leads = []

    def run(self):
        print(f"\nüîç Scraping Google for: {self.niche}")
        for dork in DEFAULT_DORKS:
            query = dork.format(niche=self.niche)
            print(f"\nüåê Query: {query}")

            try:
                urls = search(query, num_results=self.limit)
            except Exception as e:
                print(f"‚ùå Google search failed: {e}")
                continue

            for url in urls:
                print(f"‚û°Ô∏è Visiting: {url}")
                try:
                    res = requests.get(url, headers=HEADERS, timeout=10)
                    soup = BeautifulSoup(res.text, 'html.parser')
                    text = soup.get_text()
                    emails = re.findall(EMAIL_REGEX, text)
                    for email in set(emails):
                        self.save_lead(email, url)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error visiting {url}: {e}")
                time.sleep(5)  # delay to avoid being flagged

    def save_lead(self, email, source_url):
        username = email.split('@')[0]
        lead, created = Lead.objects.get_or_create(
            email=email,
            defaults={
                'username': username,
                'niche': self.niche,
                'source_url': source_url,
                'status': 'scraped'
            }
        )
        if created:
            print(f"‚úÖ Saved: {email}")
            self.scraped_leads.append({
                'username': username,
                'email': email,
                'niche': self.niche,
                'source_url': source_url
            })
        else:
            print(f"‚ö†Ô∏è Already exists: {email}")

    def export_csv(self, path):
        if not self.scraped_leads:
            print("‚ö†Ô∏è No new leads to export.")
            return

        try:
            with open(path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=['username', 'email', 'niche', 'source_url'])
                writer.writeheader()
                for lead in self.scraped_leads:
                    writer.writerow(lead)
            print(f"üìÅ Exported leads to {path}")
        except Exception as e:
            print(f"‚ùå Failed to export CSV: {e}")

